---
title: "Descriptive Statistics"
author: "Jeffrey Arnold"
date: "1/2/2018"
output: html_document
---

# Descriptive Statistics

A **statistic** is a function of data.[^sample]
That's it. Any function of data is a statistic. 
Of course, that does not mean that all statistics are equally useful for understanding data, or equally easy to work with. 


[^sample]: After introducing *sampling*, we will distinguish between **sample** and **populations**.
  In **inference**, we will use statistics of the sample to infer parameters of a population.
  However, it is useful to remember that a statistic is defined indepenently of its inferential purposes,
  even if many of the statistics that are introduced in this section and a commonly used are 
  used because of their inferrential properties.

What is the purpose of the descriptive statistics? To understand data. 
But what does it mean to *understand*? And how and what statistics aid in that?

The fundamental problem in descriptive statistics is one of *dimension reduction*.
A dataset almost certainly contains more numbers that can be held in short term memory, so how can it be reduced to a form that can be understood?
There are two main approaches:

1. *summary statistics* Reduce the data to a few numbers that represent the 
2. *data visualization* Convert the data a form (visual stimuli) that allow humans to process larger amounts .[^senses]

[^senses]: More generally, the idea is to convert data to a form that human senses can process rapidly. In practice, this is usually,
  visual. [Data sonification or aurlization](https://en.wikipedia.org/wiki/Sonification) converts data to sounds.  If one was so inclined, one could
  convert data to forms amenable to touch, taste, or smell. While these would make for interesting art projects, in most cases they seem less
  powerful than visualization. However, if data analysis was conducted by dogs, you could imagine that data olfactorization would be the primary means
  of conveying and understanding datasets.

Summary statistics and data visualization are not mutually exclusive. In particular, many data visualization methods display 
a some summary statistics of the data rather than the original data itself, with histograms and box plots being notable examples.

    
## Location

How to think about statistics? 

Claim - 

$$
mean(x) = \arg \min_m \sum_{i = 1}^n
$$
```{r}

```


We could show this via calculus.
The minimum of 
$$
\sum_{i = 1}^n
$$

$$
$$

Consider the average salary dataset from *How to Lie with Statistic*:
```{r}
library("tidyverse")
data("average_salary", package = "datums")
ggplot(average_salary, aes(x = salary)) + geom_dotplot(binwidth = 1)
```

```{r}
summary(average_salary)
```


The mean of this is the number, $m$, that minimizes $\sum_i (x_i - m)^2$.
Finding the minimum analytically with calculus allows us to both find the minimum of this function and prove it is unique.
But to help understand the objective function we'll visualize it.
For a given point $m$, calculate the total error. 
Take the actual mean $m = 5.7$:
```{r}
m <- 5.7
sum((average_salary$salary - m) ^ 2)
```
However, to show that it's.

Since we'll reuse this code, let's write a function:
```{r}
squared_error <- function(m, x) {
  sum((average_salary$salary - m) ^ 2)
}
squared_error(m, x)
```


```{r}
possible_means <-
  tibble(value = seq(min(average_salary$salary), max(average_salary$salary),
                   by = 0.1),
         err = NA)

for (i in seq_len(nrow(possible_means))) {
  possible_means[["err"]][[i]] <- squared_error(possible_means[["value"]][[i]],
                                                average_salary$salary)
}

```

We can plot the error function:
```{r}
ggplot(possible_means, aes(x = value, y = err)) +
  geom_line()
```
and we can plot the 



## Spread


## Shape
